install.packages("igraph")
install.packages("httr")
install.packages("biclust")
install.packages("SnowballC")
library(twitteR)
library(ROAuth) #login twitterlibrary
library(tm)
library(ggplot2)
library(RColorBrewer)
library(wordcloud)
library(cluster)
library(fpc)
library(igraph)
library(httr)
library(biclust)
library(SnowballC)
library(tm)
library(ggplot2)
library(RColorBrewer)
library(wordcloud)
library(cluster)
library(fpc)
library(igraph)
library(httr)
library(biclust)
library(SnowballC)
setup_twitter_oauth(Vm8DrApC9jrBk1zq8YKbBtdZV,qxSzrLGpC7OylibBTlwRAElKSIO6XSbry081ZQo87Gmgq5bMs5,
924990688692920320-jt1lv2qB4xdfRQRF0g2UVM94APuAIF5,eAghFrsI0yXrcMq1GJEqmwMKtoBKu3hei1FUYRynG9oU3)
setup_twitter_oauth(Vm8DrApC9jrBk1zq8YKbBtdZV,qxSzrLGpC7OylibBTlwRAElKSIO6XSbry081ZQo87Gmgq5bMs5,
924990688692920320-jt1lv2qB4xdfRQRF0g2UVM94APuAIF5,eAghFrsI0yXrcMq1GJEqmwMKtoBKu3hei1FUYRynG9oU3)
setup_twitter_oauth(Vm8DrApC9jrBk1zq8YKbBtdZV,qxSzrLGpC7OylibBTlwRAElKSIO6XSbry081ZQo87Gmgq5bMs5,
924990688692920320-jt1lv2qB4xdfRQRF0g2UVM94APuAIF5,eAghFrsI0yXrcMq1GJEqmwMKtoBKu3hei1FUYRynG9oU3)
library(ROAuth) #login twitterlibrary
library(twitteR)
setup_twitter_oauth(Vm8DrApC9jrBk1zq8YKbBtdZV,qxSzrLGpC7OylibBTlwRAElKSIO6XSbry081ZQo87Gmgq5bMs5,
924990688692920320-jt1lv2qB4xdfRQRF0g2UVM94APuAIF5,eAghFrsI0yXrcMq1GJEqmwMKtoBKu3hei1FUYRynG9oU3)
setup_twitter_oauth(HPJGYG2dFiTULom178mnQf2M2,
9AjlrfLdhW1wr6fcFpgVVaNFfLI4aWYel2dkcBifBZyxPeM3vo,
924990688692920320-jt1lv2qB4xdfRQRF0g2UVM94APuAIF5,
eAghFrsI0yXrcMq1GJEqmwMKtoBKu3hei1FUYRynG9oU3)
setup_twitter_oauth(HPJGYG2dFiTULom178mnQf2M2,
9AjlrfLdhW1wr6fcFpgVVaNFfLI4aWYel2dkcBifBZyxPeM3vo,
NULL,NULL)
setup_twitter_oauth(HPJGYG2dFiTULom178mnQf2M2,
9AjlrfLdhW1wr6fcFpgVVaNFfLI4aWYel2dkcBifBZyxPeM3vo,
accessToken = Null,accessSecret=Null)
setup_twitter_oauth(HPJGYG2dFiTULom178mnQf2M2,
9AjlrfLdhW1wr6fcFpgVVaNFfLI4aWYel2dkcBifBZyxPeM3vo,
accessToken = NULL,accessSecret=NULL)
library(httr)
install.packages("twitter")
install.packages("twitteR")
install.packages("twitteR")
install.packages("twitteR")
install.packages("ROAuth") #login twitterlibrary
install.packages("tm")
install.packages("ggplot2")
install.packages("RColorBrewer")
install.packages("wordcloud")
install.packages("cluster")
install.packages("fpc")
install.packages("igraph")
install.packages("httr")
install.packages("biclust")
install.packages("SnowballC")
library(twitteR)
library(ROAuth) #login twitterlibrary
library(tm)
library(ggplot2)
library(RColorBrewer)
library(wordcloud)
library(cluster)
library(fpc)
library(igraph)
library(httr)
library(biclust)
library(SnowballC)
setup_twitter_oauth(HPJGYG2dFiTULom178mnQf2M2,
9AjlrfLdhW1wr6fcFpgVVaNFfLI4aWYel2dkcBifBZyxPeM3vo,
924990688692920320-jt1lv2qB4xdfRQRF0g2UVM94APuAIF5,
eAghFrsI0yXrcMq1GJEqmwMKtoBKu3hei1FUYRynG9oU3)
setup_twitter_oauth("HPJGYG2dFiTULom178mnQf2M2",
"9AjlrfLdhW1wr6fcFpgVVaNFfLI4aWYel2dkcBifBZyxPeM3vo",
"924990688692920320-jt1lv2qB4xdfRQRF0g2UVM94APuAIF5",
"eAghFrsI0yXrcMq1GJEqmwMKtoBKu3hei1FUYRynG9oU3")
tweets<-userTimeline("dominos",n=3200)
dominos<-tweets
save(dominos,file="dominos.Rda")
saveRDS(dominos,file = "dominos.Rds")
rm(dominos)
(n.tweet<-length(tweets))
tweets.df <- twListToDF(tweets)
#tweet 190
tweets.df[190, c("id", "created", "screenName", "replyToSN", "favoriteCount", "retweetCount", "longitude", "latitude", "text")]
# print tweet #190 and make text fit for slide width
writeLines(strwrap(tweets.df$text[190], 60))
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(tweets.df$text))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# remove stopwords
myStopwords <- c(stopwords("english"), c("hacker","hack","hacking","hackers","hacks","you","could","also",
"dont","us","were","are","is","was","that","this","these","those","to",
"didnt","oh","may","a","an","of","with","will","let","wo","for","its",
"hi","o","dont","lo","if","now","in","at","next"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# keep a copy for stem completion later
myCorpusCopy <- myCorpus
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm
inspect(tdm)
(freq.terms <- findFreqTerms(tdm,lowfreq = 10))
#ggplot2
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 150)
df <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip() + theme(axis.text=element_text(size=7))
#wordcloud
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]
# plot word cloud using wordcloud
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 10, random.order = F, colors = pal)
findAssocs(tdm, "expert", 0.1)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 10, random.order = F, colors = pal)
install.packages("twitteR")
install.packages("ROAuth") #login twitterlibrary
install.packages("twitteR")
install.packages("twitteR")
install.packages("twitteR")
install.packages("twitteR")
install.packages("twitteR")
install.packages("twitteR")
install.packages("ROAuth") #login twitterlibrary
install.packages("tm")
install.packages("ggplot2")
install.packages("RColorBrewer")
install.packages("wordcloud")
install.packages("cluster")
install.packages("fpc")
install.packages("igraph")
install.packages("httr")
install.packages("httr")
install.packages("httr")
install.packages("biclust")
install.packages("SnowballC")
library(twitteR)
library(ROAuth) #login twitterlibrary
library(tm)
library(ggplot2)
library(RColorBrewer)
library(wordcloud)
library(cluster)
library(fpc)
library(igraph)
library(httr)
library(biclust)
library(SnowballC)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 10, random.order = F, colors = pal)
myStopwords <- c(stopwords("english"), c("hacker","hack","hacking","hackers","hacks","you","could","also",
"dont","us","were","are","is","was","that","this","these","those","to",
"didnt","oh","may","a","an","of","with","will","let","wo","for","its",
"hi","o","dont","lo","if","now","in","at","next","you","we","that","did",
"were","oh","get","thats","my","okay","just","never","havent","dm","amp",
"i","ff","take","th","ff","pls","letting"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
inspect(tdm)
myStopwords <- c(stopwords("english"), c("hacker","hack","hacking","hackers","hacks","you","could","also",
"dont","us","were","are","is","was","that","this","these","those","to",
"didnt","oh","may","a","an","of","with","will","let","wo","for","its",
"hi","o","dont","lo","if","now","in","at","next","you","we","that","did",
"were","oh","get","thats","my","okay","just","never","havent","dm","amp",
"i","ff","take","th","ff","pls","letting","can"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm
inspect(tdm)
(freq.terms <- findFreqTerms(tdm,lowfreq = 10))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
pal <- brewer.pal(9, "BuGn")[-(1:4)]
# plot word cloud using wordcloud
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 10, random.order = F, colors = pal)
tweets.df <- twListToDF(tweets)
#tweet 190
tweets.df[190, c("id", "created", "screenName", "replyToSN", "favoriteCount", "retweetCount", "longitude", "latitude", "text")]
writeLines(strwrap(tweets.df$text[190], 60))
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
myCorpus <- tm_map(myCorpus, stripWhitespace)
myStopwords <- c(stopwords("english"), c("hacker","hack","hacking","hackers","hacks","you","could","also",
"dont","us","were","are","is","was","that","this","these","those","to",
"didnt","oh","may","a","an","of","with","will","let","wo","for","its",
"hi","o","dont","lo","if","now","in","at","next","you","we","that","did",
"were","oh","get","thats","my","okay","just","never","havent","dm","amp",
"i","ff","take","th","ff","pls","letting","can","shouldent","enable",
"ds","ev","jc","kv","oh"))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myCorpusCopy <- myCorpus
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
tdm
inspect(tdm)
(freq.terms <- findFreqTerms(tdm,lowfreq = 10))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 150)
df <- data.frame(term = names(term.freq), freq = term.freq)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") + xlab("Terms") + ylab("Count") + coord_flip() + theme(axis.text=element_text(size=7))
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m), decreasing = T)
pal <- brewer.pal(9, "BuGn")[-(1:4)]
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 10, random.order = F, colors = pal)
getwd()
setwd("Documents/info264 wo/fianl project/")
myData<-read.csv("fianl data(Domino).csv")
View(myData)
plot(myData$International.stores,myData$International.franchise,
main = "INTERNATIONAL FRANCHISE STORES",xlab = "International franchise stores",ylab = "International franchise stores' Revenue")
fit<-lm(myData$International.franchise~myData$International.stores)
abline(fit)
fit
summary(fit)
setwd("Documents/info264 wo/final project")
getwd()
install.packages("SnowballC")
install.packages("biclust")
library(tm)
library(ggplot2)
library(RColorBrewer)
library(wordcloud)
library(cluster)
library(fpc)
library(igraph)
library(SnowballC)
library(biclust)
file.path <- "AIA2016.txt"
text <- readLines(file.path)
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function(x,pattern)gsub(pattern,"",x))
docs <- tm_map(docs,toSpace,"/")
docs <- tm_map(docs,toSpace,"@")
docs <- tm_map(docs,toSpace,"\\|")
docs <- tm_map(docs, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removeWords,stopwords("english"))
docs <- tm_map(docs,removeWords,c("the","that","this","who","where","when",
"which","a","able","about","above","all","i"
,"so","see","an","are","is","was","were","be",
"them","then","c","b","t","value","new","and",
"may","will","can","may","also","within","fair",
"use","per","change","used","made","current",
"changes","ended"))
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing = TRUE)
d <- data.frame(word=names(v),freq=v)
head(d,10)
(freq.terms <- findFreqTerms(dtm,lowfreq = 20))
set.seed(1234)
wordcloud(words = d$word,freq = d$freq,min.freq = 1,max.words = 200,
random.order = FALSE,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))
tdm <- TermDocumentMatrix(docs,control = list(wordLengths=c(1,Inf)))
tdm
inspect(tdm)
(freq.terms <- findFreqTerms(tdm, lowfreq = 10))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq,term.freq>=80)
df <- data.frame(term=names(term.freq),freq=term.freq)
ggplot(df,aes(x=term,y=freq))+geom_bar(stat = "identity")+xlab("Terms")+
ylab("Count")+coord_flip()+theme(axis.text = element_text(size=7))
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m),decreasing = T)
pal <- brewer.pal(9,"BuGn")[-(1:4)]
wordcloud(words = names(word.freq),freq = word.freq,min.freq = 3,random.order = F,
colors = pal)
findAssocs(tdm,"access",0.4)
tdm2 <- removeSparseTerms(tdm,sparse = 0.99)
m2 <- as.matrix(tdm2)
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix,method="ward.D")
plot(fit)
rect.hclust(fit,k=6)
plot(fit)
rect.hclust(fit,k=4)
file.path <- "2016_Annual_Report_DPZ.txt"
text <- readLines(file.path)
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function(x,pattern)gsub(pattern,"",x))
docs <- tm_map(docs,toSpace,"/")
docs <- tm_map(docs,toSpace,"@")
docs <- tm_map(docs,toSpace,"\\|")
docs <- tm_map(docs, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removeWords,stopwords("english"))
docs <- tm_map(docs,removeWords,c("the","that","this","who","where","when",
"which","a","able","about","above","all","i"
,"so","see","an","are","is","was","were","be",
"them","then","c","b","t","value","new","and",
"may","will","can","may","also","within","fair",
"use","per","change","used","made","current",
"changes","ended"))
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing = TRUE)
d <- data.frame(word=names(v),freq=v)
head(d,10)
(freq.terms <- findFreqTerms(dtm,lowfreq = 20))
set.seed(1234)
wordcloud(words = d$word,freq = d$freq,min.freq = 1,max.words = 200,
random.order = FALSE,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))
tdm <- TermDocumentMatrix(docs,control = list(wordLengths=c(1,Inf)))
tdm
inspect(tdm)
(freq.terms <- findFreqTerms(tdm, lowfreq = 10))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq,term.freq>=80)
df <- data.frame(term=names(term.freq),freq=term.freq)
ggplot(df,aes(x=term,y=freq))+geom_bar(stat = "identity")+xlab("Terms")+
ylab("Count")+coord_flip()+theme(axis.text = element_text(size=7))
m <- as.matrix(tdm)
word.freq <- sort(rowSums(m),decreasing = T)
pal <- brewer.pal(9,"BuGn")[-(1:4)]
wordcloud(words = names(word.freq),freq = word.freq,min.freq = 3,random.order = F,
colors = pal)
findAssocs(tdm,"access",0.4)
tdm2 <- removeSparseTerms(tdm,sparse = 0.99)
m2 <- as.matrix(tdm2)
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix,method="ward.D")
plot(fit)
rect.hclust(fit,k=6)
plot(fit)
rect.hclust(fit,k=4)
wordcloud(words = d$word,freq = d$freq,min.freq = 1,max.words = 200,
random.order = FALSE,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))
wordcloud(words = d$word,freq = d$freq,min.freq = 1,max.words = 200,
random.order = FALSE,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))
wordcloud(words = d$word,freq = d$freq,min.freq = 1,max.words = 200,
random.order = FALSE,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))
docs <- tm_map(docs,removeWords,c("the","that","this","who","where","when",
"which","a","able","about","above","all","i"
,"so","see","an","are","is","was","were","be",
"them","then","c","b","t","value","new","and",
"may","will","can","may","also","within","fair",
"use","per","change","used","made","current",
"changes","ended","well","form"))
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing = TRUE)
d <- data.frame(word=names(v),freq=v)
head(d,10)
(freq.terms <- findFreqTerms(dtm,lowfreq = 20))
set.seed(1234)
wordcloud(words = d$word,freq = d$freq,min.freq = 1,max.words = 200,
random.order = FALSE,rot.per = 0.35,colors = brewer.pal(8,"Dark2"))
